{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use partial correlation\n",
    "The subtypes are not independent. This can cause the BH FDR correction to perform not as desired (not sure in which direction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import brainbox as bb\n",
    "import nilearn as nil\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats as st\n",
    "from matplotlib import gridspec\n",
    "from scipy import cluster as scl\n",
    "from nilearn import plotting as nlp\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import linear_model as slin\n",
    "from statsmodels.sandbox import stats as sts\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from statsmodels.sandbox.stats import multicomp as smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Variables\n",
    "mtp_list = ['rmap_part', 'dual_regression', 'stability_maps']\n",
    "pheno_list = [\n",
    "              '/data1/abide/Pheno/combined_406.csv',\n",
    "              '/data1/abide/Pheno/site_balanced_279.csv',\n",
    "              '/data1/abide/Pheno/site_and_age_balanced_194.csv'\n",
    "              ]\n",
    "sample_names = [\n",
    "                'combined_406_sample',\n",
    "                'site_279_sample',\n",
    "                'site_age_194_sample'\n",
    "                ]\n",
    "scale_list = np.array([7, 12, 20, 36, 64])\n",
    "cov_list = ['ADOS_TOTAL', 'VIQ', 'DX_GROUP', 'ADOS_SOCOM_SEV','EYE_STATUS_AT_SCAN']\n",
    "# Select\n",
    "mtp_id = 2\n",
    "sc_id = 0\n",
    "phen_id = 0\n",
    "cov_id = 4\n",
    "# Make variables\n",
    "scale = scale_list[sc_id]\n",
    "mtp = mtp_list[mtp_id]\n",
    "name = sample_names[phen_id]\n",
    "pheno_path = pheno_list[phen_id]\n",
    "cov = cov_list[cov_id]\n",
    "# Fixed values\n",
    "prior_path = '/data1/cambridge/template/template_cambridge_basc_multiscale_sym_scale{:03d}.nii.gz'.format(scale)\n",
    "mask_path = '/data1/abide/Mask/mask_data_specific.nii.gz'\n",
    "in_path = '/data1/subtypes/serial_preps/'\n",
    "\n",
    "netstack_path = os.path.join(in_path, 'netstack_dmn_{}_{}_scale_{:03d}.npy'.format(mtp, name, scale))\n",
    "netraw_path = os.path.join(in_path, 'netstack_raw_{}_{}_scale_{:03d}.npy'.format(mtp, name, scale))\n",
    "corrmat_path = os.path.join(in_path, 'correlation_matrix_{}_{}_scale_{:03d}.npy'.format(mtp, name, scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the mask\n",
    "m_img = nib.load(mask_path)\n",
    "mask_data = m_img.get_data()\n",
    "mask = mask_data != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the phenotype data\n",
    "pheno = pd.read_csv(pheno_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add coded variable for ratio of VIQ and PIQ\n",
    "pheno['VerbRatio'] = pheno['VIQ'] / pheno['PIQ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the prior\n",
    "p_img = nib.load(prior_path)\n",
    "prior = p_img.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turn the priors into an image\n",
    "prior = nib.load(prior_path)\n",
    "prior_data = prior.get_data()\n",
    "prior_temp = np.zeros((prior_data.shape + (scale,)))\n",
    "for sc_id in range(scale):\n",
    "    tmp = np.zeros_like(prior_data)\n",
    "    tmp[prior_data==sc_id+1] = sc_id + 1\n",
    "    prior_temp[..., sc_id] = tmp\n",
    "prior_img = nib.Nifti1Image(prior_temp, affine=m_img.get_affine(), header=m_img.get_header())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define some functions and the niak colormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a new colormap\n",
    "cdict = {'red':   ((0.0, 0.0, 0.0),\n",
    "                   (0.5, 0.0, 0.0),\n",
    "                   (0.75, 1.0, 1.0),\n",
    "                   (1.0, 1.0, 1.0)),\n",
    "\n",
    "         'green': ((0.0, 1.0, 1.0),\n",
    "                   (0.25, 0.0, 0.0),\n",
    "                   (0.5, 0.0, 0.0),\n",
    "                   (0.75, 0.0, 0.0),\n",
    "                   (1.0, 1.0, 1.0)),\n",
    "\n",
    "         'blue':  ((0.0, 1.0, 1.0),\n",
    "                   (0.25, 1.0, 1.0),\n",
    "                   (0.5, 0.0, 0.0),\n",
    "                   (1.0, 0.0, 0.0))\n",
    "        }\n",
    "hotcold = LinearSegmentedColormap('hotcold', cdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the subtypes and compute the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the serialized netstack\n",
    "netstack = np.load(netstack_path)\n",
    "corr_mat = np.load(corrmat_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make the grand average\n",
    "gdavg = np.zeros(mask.shape + (scale,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "netraw = np.load(netraw_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100.0 % done 0.00 seconds to go. One step takes 3.50161 and we ran for 28.02 s so far"
     ]
    }
   ],
   "source": [
    "subtypes = 10\n",
    "\n",
    "scale = netstack.shape[0]\n",
    "n_sub = netstack.shape[2]\n",
    "n_vox = netstack.shape[1]\n",
    "\n",
    "link_store = np.zeros((n_sub-1,4,scale))\n",
    "part_store = np.zeros((scale, n_sub))\n",
    "sbt_store = np.zeros((scale, subtypes, n_vox))\n",
    "weight_store = np.zeros((scale, subtypes, n_sub))\n",
    "\n",
    "timer = bb.tools.Counter(scale)\n",
    "# Iterate through the networks\n",
    "for net_id in range(scale):\n",
    "    timer.tic()\n",
    "    # Compute linkage with Ward's criterion\n",
    "    link_mat = scl.hierarchy.linkage(corr_mat[net_id, ...] , method='ward')\n",
    "    link_store[..., net_id] = link_mat\n",
    "    # Partition the linkage to get a given number of subtypes\n",
    "    part_sub = scl.hierarchy.fcluster(link_mat, subtypes, criterion='maxclust')\n",
    "    part_store[net_id, :] = part_sub\n",
    "\n",
    "    sub_stack = np.zeros((n_vox, subtypes))\n",
    "    for s_id in range(subtypes):\n",
    "        sbt = np.mean(netstack[net_id, :, part_sub==s_id+1],0)\n",
    "        sub_stack[:,s_id] = sbt\n",
    "        sbt_store[net_id, s_id, :] = sbt\n",
    "\n",
    "    # Init store - Compute the weights\n",
    "    for s_id in range(subtypes):\n",
    "        type_map = sub_stack[:, s_id]\n",
    "        weight_store[net_id, s_id, :] = np.array([np.corrcoef(type_map, netstack[net_id, :, x])[0,1] for x in range(n_sub)])\n",
    "    \n",
    "    timer.toc()\n",
    "    timer.progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show_simple(net_id, sbt_id, gdavg_img, sbt_store, view_mode='x', threshold=0.1, \n",
    "                view_range=[-70,-50, -30, -10, 10, 30, 50, 70]):\n",
    "\n",
    "    sbt = sbt_store[net_id, sbt_id,:]\n",
    "    tmp = np.zeros_like(mask, dtype=np.float)\n",
    "    tmp[mask] = sbt\n",
    "    sbt_img = nib.Nifti1Image(tmp, affine=m_img.get_affine(), header=m_img.get_header())\n",
    "    fig = plt.figure(figsize=(15,3))\n",
    "    ax = fig.add_subplot(111)\n",
    "    nlp.plot_stat_map(sbt_img, display_mode='x', threshold=threshold, \n",
    "                      cut_coords=view_range, black_bg=True,\n",
    "                     axes=ax)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run partial correlation\n",
    "Prepare the model matrix, constrain the sample and then run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the subjects that have the covariate of interest\n",
    "cov_index = pd.notnull(pheno[cov])\n",
    "cov_pheno = pheno[cov_index]\n",
    "# Generate the model matrix\n",
    "factors = ['SEX', 'AGE_AT_SCAN', 'FD_scrubbed']\n",
    "# Make dummy variables for the site factor\n",
    "site_factor = pd.get_dummies(cov_pheno['SITE_ID'])\n",
    "# Turn the first site into the intercept\n",
    "site_factor = site_factor.rename(columns={site_factor.keys()[0]: 'INTERCEPT'})\n",
    "site_factor['INTERCEPT'] = 1\n",
    "# Get the other regressors\n",
    "other_factors = cov_pheno.ix[:,factors]\n",
    "# Demean age\n",
    "other_factors['AGE_AT_SCAN'] = other_factors['AGE_AT_SCAN']-np.mean(other_factors['AGE_AT_SCAN'].values)\n",
    "# Combine the regressors and turn them into an array\n",
    "regressors = pd.concat([site_factor, other_factors], axis=1).as_matrix()\n",
    "# Get the weights for these subjects\n",
    "cov_weight = weight_store[..., cov_index.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prepare the covariate\n",
    "y_cov = cov_pheno[cov].values\n",
    "pval_store = np.zeros((scale, subtypes))\n",
    "corr_store = np.zeros((scale, subtypes))\n",
    "# Loop through the networks\n",
    "for net_id in range(scale):\n",
    "    # Get the full range of subtypes for this network\n",
    "    weights = cov_weight[net_id, :, :]\n",
    "    # Create a mask for the subtypes\n",
    "    weight_mask = np.ones(subtypes, dtype=bool)\n",
    "    # Loop through the subtypes\n",
    "    for s_id in range(subtypes):\n",
    "        # Mask the other subtype weights\n",
    "        weight_mask[s_id] = False\n",
    "        # Build the regressor matrix for this subtype by adding \n",
    "        # the other subtypes\n",
    "        x =  np.concatenate((regressors, weights[weight_mask, :].T),axis=1)\n",
    "        # Linear model for the subtype weights\n",
    "        y_sbt = weights[s_id, :]\n",
    "        \n",
    "        # Regress the models\n",
    "        m_sbt = sm.OLS(y_sbt, x)\n",
    "        m_cov = sm.OLS(y_cov, x)\n",
    "        r_sbt = m_sbt.fit()\n",
    "        r_cov = m_cov.fit()\n",
    "        # Get the residuals\n",
    "        res_sbt = r_sbt.resid\n",
    "        res_cov = r_cov.resid\n",
    "        # Get the spearman correlation between them\n",
    "        (corr, p_val) = st.spearmanr(res_sbt, res_cov)\n",
    "        # Set the mask back\n",
    "        weight_mask[s_id] = True\n",
    "        \n",
    "        # Store the results\n",
    "        pval_store[net_id, s_id] = p_val\n",
    "        corr_store[net_id, s_id] = corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pval_vec = np.reshape(pval_store, np.prod(pval_store.shape))\n",
    "pcorr_vec = smi.multipletests(pval_vec.flatten(), alpha=0.05, method='fdr_bh')\n",
    "pcorr = np.reshape(pcorr_vec[1], pval_store.shape)\n",
    "pthresh = np.reshape(pcorr_vec[0], pval_store.shape)\n",
    "# Do a bonferroni correction too\n",
    "bonfcrit = 0.05 / np.prod(pval_store.shape)\n",
    "bonfvec = pval_vec < bonfcrit\n",
    "bonfthresh = np.reshape(bonfvec, pval_store.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto display the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 10)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pthresh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, False, False, False, False, False, False,\n",
       "        False],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False]], dtype=bool)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pthresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing is significant\n"
     ]
    }
   ],
   "source": [
    "# Find hits\n",
    "do_print = False\n",
    "net_hits, sbt_hits = np.where(pthresh)\n",
    "# Check if there is anything\n",
    "if net_hits.size:\n",
    "    # Find the unique networks that have hit\n",
    "    nets = np.unique(net_hits)\n",
    "    # Loop through the unique networks to have\n",
    "    # an external loop\n",
    "    for net in nets:\n",
    "        # Announce the current network\n",
    "        if do_print:\n",
    "            print('Now showing results for network {}'.format(net))\n",
    "        # Display the current network\n",
    "        if not do_print:\n",
    "            nlp.plot_glass_brain(nil.image.index_img(prior_img, net), cmap=plt.cm.spectral, \n",
    "                                 colorbar=False, vmin=1, vmax=scale,\n",
    "                                title='Network {}'.format(net))\n",
    "        # Find the indices that have results from\n",
    "        # this network\n",
    "        net_idx = np.where(net_hits==net)[0]\n",
    "        # Go through these hits on the subtype level\n",
    "        for idx in net_idx:\n",
    "            # Get the corresponding subtype\n",
    "            sbt = sbt_hits[idx]\n",
    "            # Get the corresponding partial correlation result\n",
    "            corr = corr_store[net][sbt]\n",
    "            # code orientation\n",
    "            if corr < 0:\n",
    "                orient = 'negative'\n",
    "            else: \n",
    "                orient = 'positive'\n",
    "            # Announce result\n",
    "            if do_print:\n",
    "                print('   subtype {} of network {} has a {} relationship with {}'.format(sbt, net, orient, cov))\n",
    "            # Show the result\n",
    "            if do_print:\n",
    "                print(res.summary2())\n",
    "            # Show the subtype\n",
    "            sbt_map = sbt_store[net, sbt,:]\n",
    "            tmp = np.zeros_like(mask, dtype=np.float)\n",
    "            tmp[mask] = sbt_map\n",
    "            sbt_img = nib.Nifti1Image(tmp, affine=m_img.get_affine(), header=m_img.get_header())\n",
    "            if not do_print:\n",
    "                nlp.plot_stat_map(sbt_img, display_mode='x', threshold=0, \n",
    "                              cut_coords=[-70,-50, -30, -10, 10, 30, 50, 70], black_bg=True,\n",
    "                                 title='{} {} with sbt {} @ net{})'.format(cov, orient, sbt, net))\n",
    "else:\n",
    "    print('Nothing is significant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
