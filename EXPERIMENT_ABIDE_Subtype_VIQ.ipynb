{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABIDE Subtype analysis\n",
    "The goal of this notebook is to run a subtype analysis on the ABIDE dataset and to look at the subtype weights following Pierre Orban's analysis. Here are the central research questions:\n",
    "\n",
    "1. What are subtypes of scores maps for different networks (7 network resolution on Cambridge) and how do they look like?\n",
    "2. What are the individual subject weights for these subtypes (violin plots and Pierre's matrix)\n",
    "3. For which networks and subtypes is diagnosis a significant predictor of weights?\n",
    "4. What do weights for these subtypes and networks look like for patients and healthy controls?\n",
    "5. For which networks and subtypes are dimensional scores like IQ significant predictors of weights?\n",
    "5. What does the scatterplot of weights over IQ look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import brainbox as bb\n",
    "import nilearn as nil\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats as st\n",
    "from matplotlib import gridspec\n",
    "from scipy import cluster as scl\n",
    "from nilearn import plotting as nlp\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import linear_model as slin\n",
    "from statsmodels.sandbox import stats as sts\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from statsmodels.sandbox.stats import multicomp as smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Paths\n",
    "scale = 7\n",
    "name = 'n308_sample'\n",
    "subtypes = 2\n",
    "mtp = 'stability_maps'\n",
    "prior_path = '/data1/cambridge/template/template_cambridge_basc_multiscale_sym_scale{:03d}.nii.gz'.format(scale)\n",
    "pheno_path = '/data1/abide/Pheno/unconstrained_2box_308_sample.csv'\n",
    "netstack_path = '/data1/subtypes/serial_preps/netstack_demeaned_{}_scale_{:03d}.npy'.format(name, scale)\n",
    "netraw_path = '/data1/subtypes/serial_preps/netstack_raw_{}_scale_{:03d}.npy'.format(name, scale)\n",
    "corrmat_path = '/data1/subtypes/serial_preps/correlation_matrix_{}_scale_{:03d}.npy'.format(name, scale)\n",
    "mask_path = '/data1/abide/Mask/mask_data_specific.nii.gz'\n",
    "out_path = '/data1/subtypes/sc12'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the mask\n",
    "m_img = nib.load(mask_path)\n",
    "mask_data = m_img.get_data()\n",
    "mask = mask_data != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the phenotype data\n",
    "pheno = pd.read_csv(pheno_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add coded variable for ratio of VIQ and PIQ\n",
    "pheno['VerbRatio'] = pheno['VIQ'] / pheno['PIQ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the prior\n",
    "p_img = nib.load(prior_path)\n",
    "prior = p_img.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turn the priors into an image\n",
    "prior = nib.load(prior_path)\n",
    "prior_data = prior.get_data()\n",
    "prior_temp = np.zeros((prior_data.shape + (scale,)))\n",
    "for sc_id in range(scale):\n",
    "    tmp = np.zeros_like(prior_data)\n",
    "    tmp[prior_data==sc_id+1] = sc_id + 1\n",
    "    prior_temp[..., sc_id] = tmp\n",
    "prior_img = nib.Nifti1Image(prior_temp, affine=m_img.get_affine(), header=m_img.get_header())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define some functions and the niak colormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a new colormap\n",
    "cdict = {'red':   ((0.0, 0.0, 0.0),\n",
    "                   (0.5, 0.0, 0.0),\n",
    "                   (0.75, 1.0, 1.0),\n",
    "                   (1.0, 1.0, 1.0)),\n",
    "\n",
    "         'green': ((0.0, 1.0, 1.0),\n",
    "                   (0.25, 0.0, 0.0),\n",
    "                   (0.5, 0.0, 0.0),\n",
    "                   (0.75, 0.0, 0.0),\n",
    "                   (1.0, 1.0, 1.0)),\n",
    "\n",
    "         'blue':  ((0.0, 1.0, 1.0),\n",
    "                   (0.25, 1.0, 1.0),\n",
    "                   (0.5, 0.0, 0.0),\n",
    "                   (1.0, 0.0, 0.0))\n",
    "        }\n",
    "hotcold = LinearSegmentedColormap('hotcold', cdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show_subtype(net_id, sbt_id, gdavg_img, sbt_store,\n",
    "                 sbt_store_demeaned, view_mode='x', threshold=0.1, \n",
    "                 view_range=[-70,-50, -30, -10, 10, 30, 50, 70]):\n",
    "    # Show the subtype\n",
    "    fig = plt.figure(figsize=(24,50))\n",
    "    gs = gridspec.GridSpec(12, 2, hspace=0.3)\n",
    "\n",
    "    ax1 = fig.add_subplot(gs[0,0])\n",
    "    nlp.plot_glass_brain(nil.image.index_img(gdavg_img, net_id), cmap=plt.cm.gnuplot, \n",
    "                             axes=ax1, colorbar=False, title='Scores average network {}'.format(net_id+1),\n",
    "                            vmin=0, vmax=1)\n",
    "\n",
    "    ax2 = fig.add_subplot(gs[0,1])\n",
    "    # Turn the vectorized subtype into a volume\n",
    "    sbt = sbt_store[net_id,sbt_id,:]\n",
    "    tmp = np.zeros_like(mask, dtype=np.float)\n",
    "    tmp[mask] = sbt\n",
    "    sbt_img = nib.Nifti1Image(tmp, affine=m_img.get_affine(), header=m_img.get_header())\n",
    "    nlp.plot_glass_brain(sbt_img, cmap=plt.cm.gnuplot, \n",
    "                         axes=ax2, colorbar=True, title='Subtype {} network {}'.format(sbt_id+1, net_id+1),\n",
    "                        vmin=0, vmax=1)\n",
    "    \n",
    "    ax3 = fig.add_subplot(gs[1,:])\n",
    "    # Turn the vectorized demeaned subtype into a volume\n",
    "    sbt_dm = sbt_store_demeaned[net_id,sbt_id,:]\n",
    "    tmp = np.zeros_like(mask, dtype=np.float)\n",
    "    tmp[mask] = sbt_dm\n",
    "    sbt_dm_img = nib.Nifti1Image(tmp, affine=m_img.get_affine(), header=m_img.get_header())\n",
    "    nlp.plot_stat_map(sbt_dm_img, axes=ax3, display_mode=view_mode, threshold=threshold, \n",
    "                      cut_coords=view_range, black_bg=True)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def report_weights(net_id, sbt_id, sub_asd, sub_tdc, weight_store):\n",
    "    # Look at the weights for subtype 5\n",
    "    sbt_weights = weight_store[net_id, sbt_id,:]\n",
    "    # Get the weights for the asd and tdc cases\n",
    "    asd_weights = sbt_weights[sub_asd]\n",
    "    tdc_weights = sbt_weights[sub_tdc]\n",
    "    n_asd = np.sum(sub_asd)\n",
    "    n_tdc = np.sum(sub_tdc)\n",
    "    # T-test\n",
    "    [t,p] = st.ttest_ind(asd_weights, tdc_weights)\n",
    "\n",
    "    fig = plt.figure(figsize=(8,4))\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    # Indices\n",
    "    ind_asd = np.ones(n_asd) + (np.random.random(n_asd)*2-1)*0.05\n",
    "    ind_tdc = np.ones(n_tdc) + (np.random.random(n_tdc)*2-1)*0.05 + 1\n",
    "\n",
    "    ax1.plot(ind_asd, asd_weights, 'k.')\n",
    "    ax1.plot(ind_tdc, tdc_weights, 'k.')\n",
    "    ax1.boxplot([asd_weights, tdc_weights])\n",
    "    ax1.set_xticklabels(['ASD', 'TDC'], rotation=60)\n",
    "    ax1.set_ylabel('weights')\n",
    "\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    tmp = ax2.violinplot([asd_weights, tdc_weights])\n",
    "    ax2.set_xticks([1,2])\n",
    "    ax2.set_xticklabels(['ASD', 'TDC'], rotation=60)\n",
    "\n",
    "    fig.suptitle('T-Test: T {:.2f}, p {:.4f}'.format(t,p))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the subtypes and compute the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the serialized netstack\n",
    "netstack = np.load(netstack_path)\n",
    "corr_mat = np.load(corrmat_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nsubtypes = 4\\n\\nscale = netstack.shape[0]\\nn_sub = netstack.shape[2]\\nn_vox = netstack.shape[1]\\n\\nlink_store = np.zeros((n_sub-1,4,scale))\\npart_store = np.zeros((scale, n_sub))\\nsbt_store = np.zeros((scale, subtypes, n_vox))\\nweight_store = np.zeros((scale, subtypes, n_sub))\\n\\n# Iterate through the networks\\nfor net_id in range(scale):\\n    # Compute linkage with Ward's criterion\\n    link_mat = scl.hierarchy.linkage(corr_mat[net_id, ...] , method='ward')\\n    link_store[..., net_id] = link_mat\\n    # Partition the linkage to get a given number of subtypes\\n    part_sub = scl.hierarchy.fcluster(link_mat, subtypes, criterion='maxclust')\\n    part_store[net_id, :] = part_sub\\n    \\n    sub_stack = np.zeros((n_vox, subtypes))\\n    for s_id in range(subtypes):\\n        sbt = np.mean(netstack[net_id, :, part_sub==s_id+1],0)\\n        sub_stack[:,s_id] = sbt\\n        sbt_store[net_id, s_id, :] = sbt\\n        \\n    # Init store - Compute the weights\\n    for s_id in range(subtypes):\\n        type_map = sub_stack[:, s_id]\\n        weight_store[net_id, s_id, :] = np.array([np.corrcoef(type_map, netstack[net_id, :, x])[0,1] for x in range(n_sub)])\\n    \\n    # Init store - Compute the weights\\n    for s_id in range(subtypes):\\n        type_map = sub_stack[:, s_id]\\n        weight_store[net_id, s_id, :] = np.array([np.corrcoef(type_map, netstack[net_id, :, x])[0,1] for x in range(n_sub)])\\n\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "subtypes = 4\n",
    "\n",
    "scale = netstack.shape[0]\n",
    "n_sub = netstack.shape[2]\n",
    "n_vox = netstack.shape[1]\n",
    "\n",
    "link_store = np.zeros((n_sub-1,4,scale))\n",
    "part_store = np.zeros((scale, n_sub))\n",
    "sbt_store = np.zeros((scale, subtypes, n_vox))\n",
    "weight_store = np.zeros((scale, subtypes, n_sub))\n",
    "\n",
    "# Iterate through the networks\n",
    "for net_id in range(scale):\n",
    "    # Compute linkage with Ward's criterion\n",
    "    link_mat = scl.hierarchy.linkage(corr_mat[net_id, ...] , method='ward')\n",
    "    link_store[..., net_id] = link_mat\n",
    "    # Partition the linkage to get a given number of subtypes\n",
    "    part_sub = scl.hierarchy.fcluster(link_mat, subtypes, criterion='maxclust')\n",
    "    part_store[net_id, :] = part_sub\n",
    "    \n",
    "    sub_stack = np.zeros((n_vox, subtypes))\n",
    "    for s_id in range(subtypes):\n",
    "        sbt = np.mean(netstack[net_id, :, part_sub==s_id+1],0)\n",
    "        sub_stack[:,s_id] = sbt\n",
    "        sbt_store[net_id, s_id, :] = sbt\n",
    "        \n",
    "    # Init store - Compute the weights\n",
    "    for s_id in range(subtypes):\n",
    "        type_map = sub_stack[:, s_id]\n",
    "        weight_store[net_id, s_id, :] = np.array([np.corrcoef(type_map, netstack[net_id, :, x])[0,1] for x in range(n_sub)])\n",
    "    \n",
    "    # Init store - Compute the weights\n",
    "    for s_id in range(subtypes):\n",
    "        type_map = sub_stack[:, s_id]\n",
    "        weight_store[net_id, s_id, :] = np.array([np.corrcoef(type_map, netstack[net_id, :, x])[0,1] for x in range(n_sub)])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Show the ordered similarity matrices\\nfig = plt.figure(figsize=(30,100))\\ngs = gridspec.GridSpec(scale, 4, hspace=0.3)\\nfor net_id in np.arange(0,scale):\\n    ax = fig.add_subplot(gs[net_id,1])\\n    ay = fig.add_subplot(gs[net_id,2:])\\n    subdend = bb.visu.add_subplot_axes(ax, [0, 0.71, 1, 0.29])\\n    submat = bb.visu.add_subplot_axes(ax, [0, 0, 1, 0.7])\\n    ax.set_axis_off()\\n    submat.set_axis_off()\\n    \\n    simmat = corr_mat[net_id, ...]\\n    link = link_store[..., net_id]\\n    hier = scl.hierarchy.dendrogram(link, ax=subdend, color_threshold=6)\\n    idx = hier['leaves']\\n    tmp = simmat[idx, :]\\n    show_mat = tmp[:, idx]\\n    submat.matshow(show_mat, vmin=-0.2, vmax=0.2, cmap=hotcold, aspect='auto')\\n    \\n    subdend.set_yticklabels([])\\n    subdend.set_xticklabels([])\\n    \\n    nlp.plot_glass_brain(nil.image.index_img(prior_img, net_id), cmap=plt.cm.spectral, \\n                         axes=ay, colorbar=False, title='Prior of network {}'.format(net_id+1),\\n                        vmin=1, vmax=12)\\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Show the ordered similarity matrices\n",
    "fig = plt.figure(figsize=(30,100))\n",
    "gs = gridspec.GridSpec(scale, 4, hspace=0.3)\n",
    "for net_id in np.arange(0,scale):\n",
    "    ax = fig.add_subplot(gs[net_id,1])\n",
    "    ay = fig.add_subplot(gs[net_id,2:])\n",
    "    subdend = bb.visu.add_subplot_axes(ax, [0, 0.71, 1, 0.29])\n",
    "    submat = bb.visu.add_subplot_axes(ax, [0, 0, 1, 0.7])\n",
    "    ax.set_axis_off()\n",
    "    submat.set_axis_off()\n",
    "    \n",
    "    simmat = corr_mat[net_id, ...]\n",
    "    link = link_store[..., net_id]\n",
    "    hier = scl.hierarchy.dendrogram(link, ax=subdend, color_threshold=6)\n",
    "    idx = hier['leaves']\n",
    "    tmp = simmat[idx, :]\n",
    "    show_mat = tmp[:, idx]\n",
    "    submat.matshow(show_mat, vmin=-0.2, vmax=0.2, cmap=hotcold, aspect='auto')\n",
    "    \n",
    "    subdend.set_yticklabels([])\n",
    "    subdend.set_xticklabels([])\n",
    "    \n",
    "    nlp.plot_glass_brain(nil.image.index_img(prior_img, net_id), cmap=plt.cm.spectral, \n",
    "                         axes=ay, colorbar=False, title='Prior of network {}'.format(net_id+1),\n",
    "                        vmin=1, vmax=12)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make the grand average\n",
    "gdavg = np.zeros(mask.shape + (scale,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "netraw = np.load(netraw_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reduce the phenotype data to those subjects \n",
    "# that actually have the covariate of interest\n",
    "cov_list = ['FIQ', 'VIQ', 'PIQ', 'VerbRatio', 'ADI_R_SOCIAL_TOTAL_A',\n",
    "            'ADI_R_VERBAL_TOTAL_BV', 'ADI_RRB_TOTAL_C', 'BMI', 'DX_GROUP']\n",
    "cov = cov_list[-1]\n",
    "cov_index = pd.notnull(pheno.replace(-9999, np.nan)[cov])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 subtypes\n",
      "    7 findings for VIQ ([array([4, 9]), array([5, 4]), array([5, 7]), array([5, 8]), array([6, 4]), array([6, 8]), array([6, 9])])\n",
      "\n",
      "15 subtypes\n",
      "    10 findings for VIQ ([array([2, 3]), array([4, 4]), array([ 4, 14]), array([5, 5]), array([5, 6]), array([ 5, 10]), array([ 5, 12]), array([6, 6]), array([ 6, 12]), array([ 6, 13])])\n",
      "\n",
      "20 subtypes\n",
      "    4 findings for VIQ ([array([5, 8]), array([ 5, 16]), array([6, 8]), array([ 6, 17])])\n",
      "\n",
      "25 subtypes\n",
      "    6 findings for VIQ ([array([ 4, 17]), array([ 4, 24]), array([5, 8]), array([ 5, 15]), array([ 5, 20]), array([ 6, 21])])\n",
      "\n",
      "30 subtypes\n",
      "    6 findings for VIQ ([array([ 4, 20]), array([ 4, 29]), array([ 5, 10]), array([ 5, 18]), array([ 5, 24]), array([ 6, 25])])\n",
      "\n",
      "35 subtypes\n",
      "    6 findings for VIQ ([array([ 4, 24]), array([ 4, 33]), array([ 5, 11]), array([ 5, 21]), array([ 5, 28]), array([ 6, 29])])\n"
     ]
    }
   ],
   "source": [
    "cov_list = ['FIQ', 'VIQ', 'PIQ', 'VerbRatio', 'ADI_R_SOCIAL_TOTAL_A',\n",
    "            'ADI_R_VERBAL_TOTAL_BV', 'ADI_RRB_TOTAL_C', 'BMI', 'DX_GROUP']\n",
    "cov = cov_list[1]\n",
    "sublist = np.int16(np.concatenate((np.arange(2,10,2), np.floor(np.logspace(1,2,6))),0))[:-2]\n",
    "\n",
    "for subtypes in np.arange(10,40,5):\n",
    "    print('\\n{} subtypes'.format(subtypes))\n",
    "    scale = netstack.shape[0]\n",
    "    n_sub = netstack.shape[2]\n",
    "    n_vox = netstack.shape[1]\n",
    "\n",
    "    link_store = np.zeros((n_sub-1,4,scale))\n",
    "    part_store = np.zeros((scale, n_sub))\n",
    "    sbt_store = np.zeros((scale, subtypes, n_vox))\n",
    "    weight_store = np.zeros((scale, subtypes, n_sub))\n",
    "\n",
    "    # Iterate through the networks\n",
    "    for net_id in range(scale):\n",
    "        # Compute linkage with Ward's criterion\n",
    "        link_mat = scl.hierarchy.linkage(corr_mat[net_id, ...] , method='ward')\n",
    "        link_store[..., net_id] = link_mat\n",
    "        # Partition the linkage to get a given number of subtypes\n",
    "        part_sub = scl.hierarchy.fcluster(link_mat, subtypes, criterion='maxclust')\n",
    "        part_store[net_id, :] = part_sub\n",
    "\n",
    "        sub_stack = np.zeros((n_vox, subtypes))\n",
    "        for s_id in range(subtypes):\n",
    "            sbt = np.mean(netstack[net_id, :, part_sub==s_id+1],0)\n",
    "            sub_stack[:,s_id] = sbt\n",
    "            sbt_store[net_id, s_id, :] = sbt\n",
    "\n",
    "        # Init store - Compute the weights\n",
    "        for s_id in range(subtypes):\n",
    "            type_map = sub_stack[:, s_id]\n",
    "            weight_store[net_id, s_id, :] = np.array([np.corrcoef(type_map, netstack[net_id, :, x])[0,1] for x in range(n_sub)])\n",
    "\n",
    "        # Init store - Compute the weights\n",
    "        for s_id in range(subtypes):\n",
    "            type_map = sub_stack[:, s_id]\n",
    "            weight_store[net_id, s_id, :] = np.array([np.corrcoef(type_map, netstack[net_id, :, x])[0,1] for x in range(n_sub)])\n",
    "\n",
    "\n",
    "    cov_index = pd.notnull(pheno.replace(-9999, np.nan)[cov])\n",
    "    cov_pheno = pheno[cov_index]\n",
    "    # Generate the model matrix\n",
    "    factors = [cov, 'SEX', 'AGE_AT_SCAN', 'FD_scrubbed']\n",
    "    # Make dummy variables for the site factor\n",
    "    site_factor = pd.get_dummies(cov_pheno['SITE_ID'])\n",
    "    # Turn the first site into the intercept\n",
    "    site_factor = site_factor.rename(columns={site_factor.keys()[0]: 'INTERCEPT'})\n",
    "    site_factor['INTERCEPT'] = 1\n",
    "    # Get the other variables\n",
    "    other_factors = cov_pheno.ix[:,factors]\n",
    "    # Turn diagnosis into [0,1] vector\n",
    "    #other_factors['DX_GROUP'] = other_factors['DX_GROUP'].values - 1\n",
    "    # Demean age\n",
    "    other_factors['AGE_AT_SCAN'] = other_factors['AGE_AT_SCAN']-np.mean(other_factors['AGE_AT_SCAN'].values)\n",
    "    # Demean the covariate\n",
    "    other_factors[cov] = other_factors[cov]-np.mean(other_factors[cov].values)\n",
    "    # Put them back together\n",
    "    glm_pheno = pd.concat([site_factor, other_factors], axis=1)\n",
    "    cov_weight = weight_store[..., cov_index.values]\n",
    "    res_store = list()\n",
    "    pval_store = np.zeros((scale, subtypes))\n",
    "    for net_id in range(scale):\n",
    "        res_list = list()\n",
    "        # Loop through the subtypes\n",
    "        for s_id in range(subtypes):\n",
    "            model = sm.OLS(cov_weight[net_id, s_id, :], glm_pheno)\n",
    "            results = model.fit()\n",
    "            # Save the p-values\n",
    "            pval_store[net_id, s_id] = results.pvalues[cov]\n",
    "            res_list.append(results)\n",
    "        res_store.append(res_list)\n",
    "    # Now look at the mask of p-values passing FDR Correction\n",
    "    pval_vec = np.reshape(pval_store, np.prod(pval_store.shape))\n",
    "    pcorr_vec = smi.multipletests(pval_vec.flatten(), alpha=0.05, method='fdr_bh')\n",
    "    # pcorr_vec = sts.multicomp.fdrcorrection0(pval_vec, 0.05)\n",
    "    # Find the hits\n",
    "    if np.sum(pcorr_vec[0]) > 0:\n",
    "        pcorr_store = np.reshape(pcorr_vec[0], pval_store.shape)\n",
    "        hits = np.argwhere(pcorr_store!=0)\n",
    "        print('    {} findings for {} ({})'.format(np.sum(pcorr_vec[0]), cov, list(hits)))\n",
    "    else:\n",
    "        print('    {} findings for {}'.format(np.sum(pcorr_vec[0]), cov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('{} has {}'.format(cov, np.sum(cov_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cov_pheno = pheno[cov_index]\n",
    "# Generate the model matrix\n",
    "factors = [cov, 'SEX', 'AGE_AT_SCAN', 'FD_scrubbed']\n",
    "# Make dummy variables for the site factor\n",
    "site_factor = pd.get_dummies(cov_pheno['SITE_ID'])\n",
    "# Turn the first site into the intercept\n",
    "site_factor = site_factor.rename(columns={site_factor.keys()[0]: 'INTERCEPT'})\n",
    "site_factor['INTERCEPT'] = 1\n",
    "# Get the other variables\n",
    "other_factors = cov_pheno.ix[:,factors]\n",
    "# Turn diagnosis into [0,1] vector\n",
    "#other_factors['DX_GROUP'] = other_factors['DX_GROUP'].values - 1\n",
    "# Demean age\n",
    "other_factors['AGE_AT_SCAN'] = other_factors['AGE_AT_SCAN']-np.mean(other_factors['AGE_AT_SCAN'].values)\n",
    "# Demean the covariate\n",
    "other_factors[cov] = other_factors[cov]-np.mean(other_factors[cov].values)\n",
    "# Put them back together\n",
    "glm_pheno = pd.concat([site_factor, other_factors], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(glm_pheno, aspect=0.1, interpolation='None', cmap=plt.cm.Greys_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cov_weight = weight_store[..., cov_index.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res_store = list()\n",
    "pval_store = np.zeros((scale, subtypes))\n",
    "for net_id in range(scale):\n",
    "    res_list = list()\n",
    "    # Loop through the subtypes\n",
    "    for s_id in range(subtypes):\n",
    "        model = sm.OLS(cov_weight[net_id, s_id, :], glm_pheno)\n",
    "        results = model.fit()\n",
    "        # Save the p-values\n",
    "        pval_store[net_id, s_id] = results.pvalues[cov]\n",
    "        res_list.append(results)\n",
    "    res_store.append(res_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Look at the uncorrected p-values for the GLM with diagnosis across networks\n",
    "tmp = plt.matshow(-np.log10(pval_store))\n",
    "tmp = plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now look at the mask of p-values passing FDR Correction\n",
    "pval_vec = np.reshape(pval_store, np.prod(pval_store.shape))\n",
    "pcorr_vec = sts.multicomp.fdrcorrection0(pval_vec, 0.05)\n",
    "pcorr_store = np.reshape(pcorr_vec[0], pval_store.shape)\n",
    "tmp = plt.matshow(pcorr_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(res_store[4][1].summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(res_store[4][2].summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gdavg_img = nib.Nifti1Image(gdavg, affine=m_img.get_affine(), header=m_img.get_header())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# vr = np.concatenate((np.arange(-55,-35,5), np.arange(40,55,5)))\n",
    "vr = np.array([-55,-50,-45,-40,0,40,45,50,55])\n",
    "fig_n2_sbt_5 = show_subtype(4, 1, gdavg_img, sbt_store_2, sbt_store, threshold=0.02, view_range=vr, view_mode='x')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
