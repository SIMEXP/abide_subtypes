{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABIDE Subtype analysis\n",
    "Lazy man result overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import brainbox as bb\n",
    "import nilearn as nil\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats as st\n",
    "from matplotlib import gridspec\n",
    "from scipy import cluster as scl\n",
    "from nilearn import plotting as nlp\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn import linear_model as slin\n",
    "from statsmodels.sandbox import stats as sts\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from statsmodels.sandbox.stats import multicomp as smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Variables\n",
    "mtp_list = ['rmap_part', 'dual_regression', 'stability_maps']\n",
    "pheno_list = [\n",
    "              '/data1/abide/Pheno/combined_406.csv',\n",
    "              '/data1/abide/Pheno/site_balanced_279.csv',\n",
    "              '/data1/abide/Pheno/site_and_age_balanced_194.csv'\n",
    "              ]\n",
    "sample_names = [\n",
    "                'combined_406_sample',\n",
    "                'site_279_sample',\n",
    "                'site_age_194_sample'\n",
    "                ]\n",
    "scale_list = np.array([7, 12, 20, 36, 64])\n",
    "cov_list = ['VIQ', 'DX_GROUP', 'ADOS_SOCOM_SEV','EYE_STATUS_AT_SCAN']\n",
    "# Select\n",
    "mtp_id = 1\n",
    "sc_id = 2\n",
    "phen_id = 1\n",
    "cov_id = 2\n",
    "# Make variables\n",
    "scale = scale_list[sc_id]\n",
    "mtp = mtp_list[mtp_id]\n",
    "name = sample_names[phen_id]\n",
    "pheno_path = pheno_list[phen_id]\n",
    "cov = cov_list[cov_id]\n",
    "# Fixed values\n",
    "prior_path = '/data1/cambridge/template/template_cambridge_basc_multiscale_sym_scale{:03d}.nii.gz'.format(scale)\n",
    "mask_path = '/data1/abide/Mask/mask_data_specific.nii.gz'\n",
    "in_path = '/data1/subtypes/serial_preps/'\n",
    "\n",
    "netstack_path = os.path.join(in_path, 'netstack_dmn_{}_{}_scale_{:03d}.npy'.format(mtp, name, scale))\n",
    "netraw_path = os.path.join(in_path, 'netstack_raw_{}_{}_scale_{:03d}.npy'.format(mtp, name, scale))\n",
    "corrmat_path = os.path.join(in_path, 'correlation_matrix_{}_{}_scale_{:03d}.npy'.format(mtp, name, scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the mask\n",
    "m_img = nib.load(mask_path)\n",
    "mask_data = m_img.get_data()\n",
    "mask = mask_data != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the phenotype data\n",
    "pheno = pd.read_csv(pheno_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add coded variable for ratio of VIQ and PIQ\n",
    "pheno['VerbRatio'] = pheno['VIQ'] / pheno['PIQ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the prior\n",
    "p_img = nib.load(prior_path)\n",
    "prior = p_img.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turn the priors into an image\n",
    "prior = nib.load(prior_path)\n",
    "prior_data = prior.get_data()\n",
    "prior_temp = np.zeros((prior_data.shape + (scale,)))\n",
    "for sc_id in range(scale):\n",
    "    tmp = np.zeros_like(prior_data)\n",
    "    tmp[prior_data==sc_id+1] = sc_id + 1\n",
    "    prior_temp[..., sc_id] = tmp\n",
    "prior_img = nib.Nifti1Image(prior_temp, affine=m_img.get_affine(), header=m_img.get_header())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define some functions and the niak colormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the subtypes and compute the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '/data1/subtypes/serial_preps/netstack_dmn_dual_regression_site_279_sample_scale_020.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-24da4d74717f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load the serialized netstack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnetstack\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetstack_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mcorr_mat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrmat_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/numpy/lib/npyio.pyc\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    360\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/data1/subtypes/serial_preps/netstack_dmn_dual_regression_site_279_sample_scale_020.npy'"
     ]
    }
   ],
   "source": [
    "# Load the serialized netstack\n",
    "netstack = np.load(netstack_path)\n",
    "corr_mat = np.load(corrmat_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make the grand average\n",
    "gdavg = np.zeros(mask.shape + (scale,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "netraw = np.load(netraw_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subtypes = 5\n",
    "\n",
    "scale = netstack.shape[0]\n",
    "n_sub = netstack.shape[2]\n",
    "n_vox = netstack.shape[1]\n",
    "\n",
    "link_store = np.zeros((n_sub-1,4,scale))\n",
    "part_store = np.zeros((scale, n_sub))\n",
    "sbt_store = np.zeros((scale, subtypes, n_vox))\n",
    "weight_store = np.zeros((scale, subtypes, n_sub))\n",
    "\n",
    "# Iterate through the networks\n",
    "for net_id in range(scale):\n",
    "    # Compute linkage with Ward's criterion\n",
    "    link_mat = scl.hierarchy.linkage(corr_mat[net_id, ...] , method='ward')\n",
    "    link_store[..., net_id] = link_mat\n",
    "    # Partition the linkage to get a given number of subtypes\n",
    "    part_sub = scl.hierarchy.fcluster(link_mat, subtypes, criterion='maxclust')\n",
    "    part_store[net_id, :] = part_sub\n",
    "\n",
    "    sub_stack = np.zeros((n_vox, subtypes))\n",
    "    for s_id in range(subtypes):\n",
    "        sbt = np.mean(netstack[net_id, :, part_sub==s_id+1],0)\n",
    "        sub_stack[:,s_id] = sbt\n",
    "        sbt_store[net_id, s_id, :] = sbt\n",
    "\n",
    "    # Init store - Compute the weights\n",
    "    for s_id in range(subtypes):\n",
    "        type_map = sub_stack[:, s_id]\n",
    "        weight_store[net_id, s_id, :] = np.array([np.corrcoef(type_map, netstack[net_id, :, x])[0,1] for x in range(n_sub)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show_simple(net_id, sbt_id, gdavg_img, sbt_store, view_mode='x', threshold=0.1, \n",
    "                view_range=[-70,-50, -30, -10, 10, 30, 50, 70]):\n",
    "\n",
    "    sbt = sbt_store[net_id, sbt_id,:]\n",
    "    tmp = np.zeros_like(mask, dtype=np.float)\n",
    "    tmp[mask] = sbt\n",
    "    sbt_img = nib.Nifti1Image(tmp, affine=m_img.get_affine(), header=m_img.get_header())\n",
    "    fig = plt.figure(figsize=(15,3))\n",
    "    ax = fig.add_subplot(111)\n",
    "    nlp.plot_stat_map(sbt_img, display_mode='x', threshold=threshold, \n",
    "                      cut_coords=view_range, black_bg=True,\n",
    "                     axes=ax)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the GLM\n",
    "Prepare the model matrix, constrain the sample and then run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the subjects that have the covariate\n",
    "cov_index = pd.notnull(pheno[cov])\n",
    "cov_pheno = pheno[cov_index]\n",
    "# Generate the model matrix\n",
    "factors = [cov, 'SEX', 'AGE_AT_SCAN', 'FD_scrubbed']\n",
    "# Make dummy variables for the site factor\n",
    "site_factor = pd.get_dummies(cov_pheno['SITE_ID'])\n",
    "# Turn the first site into the intercept\n",
    "site_factor = site_factor.rename(columns={site_factor.keys()[0]: 'INTERCEPT'})\n",
    "site_factor['INTERCEPT'] = 1\n",
    "# Get the other variables\n",
    "other_factors = cov_pheno.ix[:,factors]\n",
    "# Turn diagnosis into [0,1] vector\n",
    "if cov == 'DX_GROUP':\n",
    "    other_factors[cov] = other_factors[cov].values - 1\n",
    "else:\n",
    "    # Demean the covariate\n",
    "    other_factors[cov] = other_factors[cov]-np.mean(other_factors[cov].values)\n",
    "\n",
    "# Demean age\n",
    "other_factors['AGE_AT_SCAN'] = other_factors['AGE_AT_SCAN']-np.mean(other_factors['AGE_AT_SCAN'].values)\n",
    "# Put them back together\n",
    "glm_pheno = pd.concat([site_factor, other_factors], axis=1)\n",
    "# Get the weights for these subjects\n",
    "cov_weight = weight_store[..., cov_index.values]\n",
    "\n",
    "# Prepare storage variables\n",
    "res_store = list()\n",
    "pval_store = np.zeros((scale, subtypes))\n",
    "for net_id in range(scale):\n",
    "    res_list = list()\n",
    "    # Loop through the subtypes\n",
    "    for s_id in range(subtypes):\n",
    "        model = sm.OLS(cov_weight[net_id, s_id, :], glm_pheno)\n",
    "        results = model.fit()\n",
    "        # Save the p-values\n",
    "        pval_store[net_id, s_id] = results.pvalues[cov]\n",
    "        res_list.append(results)\n",
    "    res_store.append(res_list)\n",
    "# Now look at the mask of p-values passing FDR Correction\n",
    "pval_vec = np.reshape(pval_store, np.prod(pval_store.shape))\n",
    "pcorr_vec = smi.multipletests(pval_vec.flatten(), alpha=0.05, method='fdr_bh')\n",
    "pcorr = np.reshape(pcorr_vec[1], pval_store.shape)\n",
    "pthresh = np.reshape(pcorr_vec[0], pval_store.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto display the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "font = {'family': 'serif',\n",
    "        'color':  'lime',\n",
    "        'weight': 'heavy',\n",
    "        'size': 18,\n",
    "        }\n",
    "txt_offset=8\n",
    "# Get the two lists of hit indices\n",
    "net_hits, sbt_hits = np.where(pthresh)\n",
    "cov_val = glm_pheno[cov].values\n",
    "# Check if there is anything\n",
    "if net_hits.size:\n",
    "    # Find the unique networks that have a hit\n",
    "    nets = np.unique(net_hits)\n",
    "    # Get the number of hits\n",
    "    n_hits = len(net_hits)\n",
    "    n_figs = n_hits+len(nets)\n",
    "    # Prepare the figure\n",
    "    fig = plt.figure(figsize=(25,6*n_figs))\n",
    "    gs = gridspec.GridSpec(n_figs, 4)\n",
    "    counter = -1\n",
    "    # Loop through the unique networks to have\n",
    "    # an external loop\n",
    "    for net in nets:\n",
    "        # Display the current network\n",
    "        counter += 1\n",
    "        ax1a = fig.add_subplot(gs[counter, 0])\n",
    "        ax1b = fig.add_subplot(gs[counter, 1:])\n",
    "        # Get the order and correlation matrix\n",
    "        link_mat = scl.hierarchy.linkage(corr_mat[net, ...] , method='ward')\n",
    "        ind = scl.hierarchy.dendrogram(link_mat, no_plot=True)['leaves']\n",
    "        tmp = corr_mat[net, ...]\n",
    "        show_mat = tmp[ind, :][:, ind]\n",
    "        # Get the partition as an overlay on the correlation matrix\n",
    "        part_test = part_store[net][ind]\n",
    "        part_frame = np.zeros(show_mat.shape)\n",
    "        text_loc = list()\n",
    "        for sbt_id in np.arange(subtypes):\n",
    "            # Get first and last element\n",
    "            hits = np.where(part_test==sbt_id+1)[0]\n",
    "            first = hits[0]\n",
    "            last = hits[-1]\n",
    "            text_loc.append((len(hits)/2 + first-txt_offset, len(hits)/2 + first+txt_offset))\n",
    "            # Draw column\n",
    "            part_frame[first:last, (first, last)] = 1\n",
    "            # Draw rows\n",
    "            part_frame[(first, last), first:last] = 1\n",
    "        part_frame = np.ma.masked_where(part_frame == 0, part_frame)\n",
    "        \n",
    "        ax1a.matshow(show_mat, vmin=-0.2, vmax=0.2, cmap=bb.visu.hot_cold())\n",
    "        ax1a.matshow(part_frame, cmap=plt.cm.Greys)\n",
    "        for sbt_id in np.arange(subtypes):\n",
    "            (x,y) = text_loc[sbt_id]\n",
    "            ax1a.text(x,y,'{}'.format(sbt_id), fontdict=font)\n",
    "        nlp.plot_glass_brain(nil.image.index_img(prior_img, net), cmap=plt.cm.spectral, \n",
    "                             colorbar=False, vmin=1, vmax=scale,axes=ax1b,\n",
    "                            title='Network {}'.format(net))\n",
    "        # Find the subtypes that belong to this network\n",
    "        sbts = sbt_hits[net_hits==net]\n",
    "        # Go through these hits on the subtype level\n",
    "        for sbt in sbts:\n",
    "            counter += 1\n",
    "            ax2a = fig.add_subplot(gs[counter, 0])\n",
    "            # Get the sbt weights\n",
    "            sbt_w = cov_weight[net, sbt, :]\n",
    "            # Plot these over the covariate\n",
    "            ax2a.scatter(cov_val, sbt_w)\n",
    "            ax2b = fig.add_subplot(gs[counter, 1:])\n",
    "            # Get the corresponding GLM result\n",
    "            res = res_store[net][sbt]\n",
    "            # Get sbt t-value\n",
    "            sbt_t = res.tvalues[cov]\n",
    "            # code orientation\n",
    "            if sbt_t < 0:\n",
    "                orient = 'negative'\n",
    "            else: \n",
    "                orient = 'positive'\n",
    "            # Show the subtype\n",
    "            sbt_map = sbt_store[net, sbt,:]\n",
    "            tmp = np.zeros_like(mask, dtype=np.float)\n",
    "            tmp[mask] = sbt_map\n",
    "            sbt_img = nib.Nifti1Image(tmp, affine=m_img.get_affine(), header=m_img.get_header())\n",
    "            nlp.plot_stat_map(sbt_img, display_mode='x', threshold=0, \n",
    "                          cut_coords=[-50, -30, -10, 10, 30, 50], black_bg=True,axes=ax2b,\n",
    "                             title='{} {} with sbt {} @ net{} (T={:.2f})'.format(cov, orient, sbt, net, sbt_t))\n",
    "else:\n",
    "    print('Nothing is significant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
