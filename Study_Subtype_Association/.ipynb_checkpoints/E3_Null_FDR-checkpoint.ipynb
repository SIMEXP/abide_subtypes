{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect Hits\n",
    "and write them out to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import brainbox as bb\n",
    "import nilearn as nil\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats as st\n",
    "from matplotlib import gridspec\n",
    "from scipy import cluster as scl\n",
    "from multiprocessing import Pool\n",
    "from nilearn import plotting as nlp\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import linear_model as slin\n",
    "from statsmodels.sandbox import stats as sts\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from statsmodels.sandbox.stats import multicomp as smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Variables\n",
    "mtp_list = ['rmap_part', 'dual_regression', 'stability_maps']\n",
    "pheno_list = [\n",
    "              '/data1/abide/Pheno/combined_406.csv',\n",
    "              '/data1/abide/Pheno/site_balanced_279.csv',\n",
    "              '/data1/abide/Pheno/site_and_age_balanced_194.csv'\n",
    "              ]\n",
    "sample_names = [\n",
    "                'combined_406_sample',\n",
    "                'site_279_sample',\n",
    "                'site_age_194_sample'\n",
    "                ]\n",
    "scale_list = np.array([7, 12, 20, 36, 64])\n",
    "#cov_list = ['VIQ', 'DX_GROUP', 'ADOS_SOCOM_SEV', 'VPR', 'EYE_STATUS_AT_SCAN']\n",
    "cov_list = ['EYE_STATUS_AT_SCAN', 'ADOS_SOCOM_SEV','DX_GROUP','VIQ']\n",
    "out_str = 'Network,VIQ,DX_GROUP,ADOS_SOCOM_SEV,VPR,EYE_STATUS_AT_SCAN'\n",
    "# Select\n",
    "n_iter = 1000\n",
    "mtp_id = 2\n",
    "sc_id = [0,1,2,3]\n",
    "phen_id = 1\n",
    "# Make variables\n",
    "scales = scale_list[sc_id]\n",
    "mtp = mtp_list[mtp_id]\n",
    "name = sample_names[phen_id]\n",
    "pheno_path = pheno_list[phen_id]\n",
    "# Fixed values\n",
    "mask_path = '/data1/abide/Mask/mask_data_specific.nii.gz'\n",
    "in_path = '/data1/subtypes/serial_preps/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the mask\n",
    "m_img = nib.load(mask_path)\n",
    "mask_data = m_img.get_data()\n",
    "mask = mask_data != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the phenotype data\n",
    "pheno = pd.read_csv(pheno_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_cases = len(pheno)\n",
    "pheno['rand'] = np.arange(n_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glm_dict = dict()\n",
    "\n",
    "for cov in cov_list:\n",
    "    cov_index = pd.notnull(pheno[cov])\n",
    "    cov_pheno = pheno[cov_index]\n",
    "    # Generate the model matrix\n",
    "    factors = [cov, 'SEX', 'AGE_AT_SCAN', 'FD_scrubbed']\n",
    "    # Make dummy variables for the site factor\n",
    "    site_factor = pd.get_dummies(cov_pheno['SITE_ID'])\n",
    "    # Turn the first site into the intercept\n",
    "    site_factor = site_factor.rename(columns={site_factor.keys()[0]: 'INTERCEPT'})\n",
    "    site_factor['INTERCEPT'] = 1\n",
    "    # Get the other variables\n",
    "    other_factors = cov_pheno.ix[:,factors]\n",
    "    # Turn diagnosis into [0,1] vector\n",
    "    #other_factors['DX_GROUP'] = other_factors['DX_GROUP'].values - 1\n",
    "    # Demean age\n",
    "    other_factors['AGE_AT_SCAN'] = other_factors['AGE_AT_SCAN']-np.mean(other_factors['AGE_AT_SCAN'].values)\n",
    "    # Demean the covariate\n",
    "    other_factors[cov] = other_factors[cov]-np.mean(other_factors[cov].values)\n",
    "    # Put them back together\n",
    "    glm_pheno = pd.concat([site_factor, other_factors], axis=1)\n",
    "    glm_dict[cov] = (glm_pheno, cov_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def par_track(args):\n",
    "    (seed, glm_pheno, cov,\n",
    "     scale, subtypes, cov_weight) = args\n",
    "    np.random.seed(seed)\n",
    "    # Randomize the covariate\n",
    "    tmp = glm_pheno[cov].values\n",
    "    np.random.shuffle(tmp)\n",
    "    glm_pheno[cov] = tmp\n",
    "\n",
    "    pvec = np.zeros(np.prod((scale, subtypes)))\n",
    "    for net_id in range(scale):\n",
    "        # Loop through the subtypes\n",
    "        tmp = np.array([sm.OLS(cov_weight[net_id, s_id, :], glm_pheno).fit().pvalues[cov] for s_id in range(subtypes)])\n",
    "        pvec[net_id*subtypes:net_id*subtypes+subtypes] = tmp \n",
    "    # Now look at the mask of p-values passing FDR Correction\n",
    "    pcorr_vec = smi.multipletests(pvec, alpha=0.05, method='fdr_bh')[0]\n",
    "    # count the results passing the threshold\n",
    "    n_res = np.sum(pcorr_vec)\n",
    "    return n_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100.0 % done 0.00 seconds to go. One step takes 12.27783 and we ran for 340.95 s so far"
     ]
    }
   ],
   "source": [
    "# Prepare storage\n",
    "findings = dict()\n",
    "for scale in scales:\n",
    "    findings[scale] = dict()\n",
    "    for cov in cov_list:\n",
    "        findings[scale][cov] = list()\n",
    "\n",
    "timer = bb.tools.Counter(np.prod((len(scales), len(cov_list))))\n",
    "for scale in scales:\n",
    "    # Scale stuff\n",
    "    netstack_path = os.path.join(in_path, 'netstack_dmn_{}_{}_scale_{:03d}.npy'.format(mtp, name, scale))\n",
    "    netraw_path = os.path.join(in_path, 'netstack_raw_{}_{}_scale_{:03d}.npy'.format(mtp, name, scale))\n",
    "    corrmat_path = os.path.join(in_path, 'correlation_matrix_{}_{}_scale_{:03d}.npy'.format(mtp, name, scale))\n",
    "    prior_path = '/data1/cambridge/template/template_cambridge_basc_multiscale_sym_scale{:03d}.nii.gz'.format(scale)\n",
    "    \n",
    "    # Get the prior\n",
    "    p_img = nib.load(prior_path)\n",
    "    prior = p_img.get_data()\n",
    "    \n",
    "    # Turn the priors into an image\n",
    "    prior = nib.load(prior_path)\n",
    "    prior_data = prior.get_data()\n",
    "    prior_temp = np.zeros((prior_data.shape + (scale,)))\n",
    "    for sc_id in range(scale):\n",
    "        tmp = np.zeros_like(prior_data)\n",
    "        tmp[prior_data==sc_id+1] = sc_id + 1\n",
    "        prior_temp[..., sc_id] = tmp\n",
    "    prior_img = nib.Nifti1Image(prior_temp, affine=m_img.get_affine(), header=m_img.get_header())\n",
    "    \n",
    "    # Load the serialized netstack\n",
    "    netstack = np.load(netstack_path)\n",
    "    corr_mat = np.load(corrmat_path)\n",
    "    \n",
    "    subtypes = 5\n",
    "\n",
    "    n_sub = netstack.shape[2]\n",
    "    n_vox = netstack.shape[1]\n",
    "\n",
    "    # Make the grand average\n",
    "    gdavg = np.zeros(mask.shape + (scale,))\n",
    "\n",
    "    scale = netstack.shape[0]\n",
    "    n_sub = netstack.shape[2]\n",
    "    n_vox = netstack.shape[1]\n",
    "\n",
    "    link_store = np.zeros((n_sub-1,4,scale))\n",
    "    part_store = np.zeros((scale, n_sub))\n",
    "    sbt_store = np.zeros((scale, subtypes, n_vox))\n",
    "    weight_store = np.zeros((scale, subtypes, n_sub))\n",
    "\n",
    "    # Iterate through the networks\n",
    "    for net_id in range(scale):\n",
    "        # Compute linkage with Ward's criterion\n",
    "        link_mat = scl.hierarchy.linkage(corr_mat[net_id, ...] , method='ward')\n",
    "        link_store[..., net_id] = link_mat\n",
    "        # Partition the linkage to get a given number of subtypes\n",
    "        part_sub = scl.hierarchy.fcluster(link_mat, subtypes, criterion='maxclust')\n",
    "        part_store[net_id, :] = part_sub\n",
    "\n",
    "        sub_stack = np.zeros((n_vox, subtypes))\n",
    "        for s_id in range(subtypes):\n",
    "            sbt = np.mean(netstack[net_id, :, part_sub==s_id+1],0)\n",
    "            sub_stack[:,s_id] = sbt\n",
    "            sbt_store[net_id, s_id, :] = sbt\n",
    "\n",
    "        # Init store - Compute the weights\n",
    "        for s_id in range(subtypes):\n",
    "            type_map = sub_stack[:, s_id]\n",
    "            weight_store[net_id, s_id, :] = np.array([np.corrcoef(type_map, netstack[net_id, :, x])[0,1] for x in range(n_sub)])\n",
    "    \n",
    "    # prepare the parallel run\n",
    "    for cov in cov_list:\n",
    "        timer.tic()\n",
    "        (glm_pheno, cov_index) = glm_dict[cov]\n",
    "        cov_weight = weight_store[..., cov_index.values]\n",
    "        # Loop through the iterations\n",
    "        arg_list = list()\n",
    "        for n_it in np.arange(n_iter):\n",
    "            arg_list.append((n_it, glm_pheno, cov,\n",
    "                             scale, subtypes, cov_weight))\n",
    "        \n",
    "        # Run parallel\n",
    "        pool = Pool(processes=6)\n",
    "        results = pool.map(par_track, arg_list)\n",
    "        findings[scale][cov] = np.array(results)\n",
    "        timer.toc()\n",
    "        timer.progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a report\n",
    "report  = pd.DataFrame()\n",
    "headers = ['scale'] + cov_list\n",
    "n_scales = len(scales)\n",
    "n_covs = len(cov_list)\n",
    "\n",
    "data = np.zeros((n_scales, n_covs+1))\n",
    "data[:, 0] = np.array(scales)\n",
    "for sc_id, scale in enumerate(scales):\n",
    "    data[sc_id, 1:] = np.array([np.float(np.sum(findings[scale][cov] != 0))/n_iter for cov in cov_list])\n",
    "    \n",
    "report = pd.DataFrame(data, columns=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scale</th>\n",
       "      <th>EYE_STATUS_AT_SCAN</th>\n",
       "      <th>ADOS_SOCOM_SEV</th>\n",
       "      <th>DX_GROUP</th>\n",
       "      <th>VIQ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   scale  EYE_STATUS_AT_SCAN  ADOS_SOCOM_SEV  DX_GROUP    VIQ\n",
       "0      7               0.040           0.055     0.038  0.048\n",
       "1     12               0.041           0.044     0.036  0.045\n",
       "2     20               0.049           0.044     0.034  0.038\n",
       "3     36               0.044           0.038     0.043  0.034"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The empirical FDR is actually very close to the selected q-Value. So I guess we can at least say that my analysis doesn't create spurious significant results in the absence of true signal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
