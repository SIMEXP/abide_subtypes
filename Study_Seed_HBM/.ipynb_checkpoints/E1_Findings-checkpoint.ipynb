{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1 - Findings\n",
    "List the findings for seed maps at the scale levels and for the covariates of \n",
    "- Diagnosis\n",
    "- ADOS severity\n",
    "- VIQ\n",
    "- VIQ/PIQ ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import brainbox as bb\n",
    "import nilearn as nil\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats as st\n",
    "from matplotlib import gridspec\n",
    "from scipy import cluster as scl\n",
    "from nilearn import plotting as nlp\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import linear_model as slin\n",
    "from statsmodels.sandbox import stats as sts\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from statsmodels.sandbox.stats import multicomp as smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Variables\n",
    "scale_list = np.array([7, 12, 20, 36, 64])\n",
    "#cov_list = ['DX_GROUP', 'ADOS_SOCOM_SEV', 'VIQ', 'VPR', 'EYE_STATUS_AT_SCAN']\n",
    "cov_list = ['DX_GROUP', 'ADOS_SOCOM_SEV', 'VIQ', 'VPR']\n",
    "#sc_id = [0,1,2,3]\n",
    "sc_id = [0]\n",
    "# Make variables\n",
    "scales = scale_list[sc_id]\n",
    "mtp = 'rmap_part'\n",
    "name = 'site_279_sample'\n",
    "pheno_path = '/data1/abide/Pheno/site_balanced_279.csv'\n",
    "# Fixed values\n",
    "mask_path = '/data1/abide/Mask/mask_data_specific.nii.gz'\n",
    "in_path = '/data1/subtypes/serial_preps/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the mask\n",
    "m_img = nib.load(mask_path)\n",
    "mask_data = m_img.get_data()\n",
    "mask = mask_data != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the phenotype data\n",
    "pheno = pd.read_csv(pheno_path)\n",
    "pheno['VPR'] = pheno['VIQ'].values / pheno['PIQ'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "SCALE 7 for rmap_part with site_279_sample\n",
      "\n",
      "    5 findings for DX_GROUP ([array([0, 3]), array([2, 0]), array([2, 3]), array([2, 4]), array([5, 3])])\n",
      "\n",
      "    3 findings for ADOS_SOCOM_SEV ([array([0, 1]), array([0, 3]), array([2, 0])])\n",
      "\n",
      "    0 findings for VIQ\n",
      "\n",
      "    0 findings for VPR\n"
     ]
    }
   ],
   "source": [
    "for scale in scales:\n",
    "    print('\\n\\nSCALE {} for {} with {}'.format(scale, mtp, name))\n",
    "    # Scale stuff\n",
    "    netstack_path = os.path.join(in_path, 'netstack_dmn_{}_{}_scale_{:03d}.npy'.format(mtp, name, scale))\n",
    "    netraw_path = os.path.join(in_path, 'netstack_raw_{}_{}_scale_{:03d}.npy'.format(mtp, name, scale))\n",
    "    corrmat_path = os.path.join(in_path, 'correlation_matrix_{}_{}_scale_{:03d}.npy'.format(mtp, name, scale))\n",
    "    prior_path = '/data1/cambridge/template/template_cambridge_basc_multiscale_sym_scale{:03d}.nii.gz'.format(scale)\n",
    "    \n",
    "    # Get the prior\n",
    "    p_img = nib.load(prior_path)\n",
    "    prior = p_img.get_data()\n",
    "    \n",
    "    # Turn the priors into an image\n",
    "    prior = nib.load(prior_path)\n",
    "    prior_data = prior.get_data()\n",
    "    prior_temp = np.zeros((prior_data.shape + (scale,)))\n",
    "    for sc_id in range(scale):\n",
    "        tmp = np.zeros_like(prior_data)\n",
    "        tmp[prior_data==sc_id+1] = sc_id + 1\n",
    "        prior_temp[..., sc_id] = tmp\n",
    "    prior_img = nib.Nifti1Image(prior_temp, affine=m_img.get_affine(), header=m_img.get_header())\n",
    "    \n",
    "    # Load the serialized netstack\n",
    "    netstack = np.load(netstack_path)\n",
    "    corr_mat = np.load(corrmat_path)\n",
    "    \n",
    "    subtypes = 5\n",
    "\n",
    "    n_sub = netstack.shape[2]\n",
    "    n_vox = netstack.shape[1]\n",
    "\n",
    "    # Make the grand average\n",
    "    gdavg = np.zeros(mask.shape + (scale,))\n",
    "\n",
    "    scale = netstack.shape[0]\n",
    "    n_sub = netstack.shape[2]\n",
    "    n_vox = netstack.shape[1]\n",
    "\n",
    "    link_store = np.zeros((n_sub-1,4,scale))\n",
    "    part_store = np.zeros((scale, n_sub))\n",
    "    sbt_store = np.zeros((scale, subtypes, n_vox))\n",
    "    weight_store = np.zeros((scale, subtypes, n_sub))\n",
    "\n",
    "    # Iterate through the networks\n",
    "    for net_id in range(scale):\n",
    "        # Compute linkage with Ward's criterion\n",
    "        link_mat = scl.hierarchy.linkage(corr_mat[net_id, ...] , method='ward')\n",
    "        link_store[..., net_id] = link_mat\n",
    "        # Partition the linkage to get a given number of subtypes\n",
    "        part_sub = scl.hierarchy.fcluster(link_mat, subtypes, criterion='maxclust')\n",
    "        part_store[net_id, :] = part_sub\n",
    "\n",
    "        sub_stack = np.zeros((n_vox, subtypes))\n",
    "        for s_id in range(subtypes):\n",
    "            sbt = np.mean(netstack[net_id, :, part_sub==s_id+1],0)\n",
    "            sub_stack[:,s_id] = sbt\n",
    "            sbt_store[net_id, s_id, :] = sbt\n",
    "\n",
    "        # Init store - Compute the weights\n",
    "        for s_id in range(subtypes):\n",
    "            type_map = sub_stack[:, s_id]\n",
    "            weight_store[net_id, s_id, :] = np.array([np.corrcoef(type_map, netstack[net_id, :, x])[0,1] for x in range(n_sub)])\n",
    "\n",
    "        # Init store - Compute the weights\n",
    "        for s_id in range(subtypes):\n",
    "            type_map = sub_stack[:, s_id]\n",
    "            weight_store[net_id, s_id, :] = np.array([np.corrcoef(type_map, netstack[net_id, :, x])[0,1] for x in range(n_sub)])\n",
    "\n",
    "    for cov in cov_list:\n",
    "        cov_index = pd.notnull(pheno.replace(-9999, np.nan)[cov])\n",
    "        cov_pheno = pheno[cov_index]\n",
    "        # Generate the model matrix\n",
    "        factors = [cov, 'SEX', 'AGE_AT_SCAN', 'FD_scrubbed']\n",
    "        # Make dummy variables for the site factor\n",
    "        site_factor = pd.get_dummies(cov_pheno['SITE_ID'])\n",
    "        # Turn the first site into the intercept\n",
    "        site_factor = site_factor.rename(columns={site_factor.keys()[0]: 'INTERCEPT'})\n",
    "        site_factor['INTERCEPT'] = 1\n",
    "        # Get the other variables\n",
    "        other_factors = cov_pheno.ix[:,factors]\n",
    "        # Turn diagnosis into [0,1] vector\n",
    "        #other_factors['DX_GROUP'] = other_factors['DX_GROUP'].values - 1\n",
    "        # Demean age\n",
    "        other_factors['AGE_AT_SCAN'] = other_factors['AGE_AT_SCAN']-np.mean(other_factors['AGE_AT_SCAN'].values)\n",
    "        # Demean the covariate\n",
    "        other_factors[cov] = other_factors[cov]-np.mean(other_factors[cov].values)\n",
    "        # Put them back together\n",
    "        glm_pheno = pd.concat([site_factor, other_factors], axis=1)\n",
    "        cov_weight = weight_store[..., cov_index.values]\n",
    "        res_store = list()\n",
    "        pval_store = np.zeros((scale, subtypes))\n",
    "        for net_id in range(scale):\n",
    "            res_list = list()\n",
    "            # Loop through the subtypes\n",
    "            for s_id in range(subtypes):\n",
    "                model = sm.OLS(cov_weight[net_id, s_id, :], glm_pheno)\n",
    "                results = model.fit()\n",
    "                # Save the p-values\n",
    "                pval_store[net_id, s_id] = results.pvalues[cov]\n",
    "                res_list.append(results)\n",
    "            res_store.append(res_list)\n",
    "        # Now look at the mask of p-values passing FDR Correction\n",
    "        pval_vec = np.reshape(pval_store, np.prod(pval_store.shape))\n",
    "        pcorr_vec = smi.multipletests(pval_vec.flatten(), alpha=0.05, method='fdr_bh')\n",
    "        # Do a bonferroni correction too\n",
    "        bonfcrit = 0.05 / np.prod(pval_store.shape)\n",
    "        bonfvec = pval_vec < bonfcrit\n",
    "        bonfthresh = np.reshape(bonfvec, pval_store.shape)\n",
    "\n",
    "        # pcorr_vec = sts.multicomp.fdrcorrection0(pval_vec, 0.05)\n",
    "        # Find the hits\n",
    "        if np.sum(pcorr_vec[0]) > 0:\n",
    "        #if bonfthresh.any():\n",
    "            pcorr_store = np.reshape(pcorr_vec[0], pval_store.shape)\n",
    "            hits = np.argwhere(pcorr_store!=0)\n",
    "            print('\\n    {} findings for {} ({})'.format(np.sum(pcorr_vec[0]), cov, list(hits)))\n",
    "        else:\n",
    "            print('\\n    {} findings for {}'.format(np.sum(pcorr_vec[0]), cov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 2]), array([1, 3, 0]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(pcorr_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
